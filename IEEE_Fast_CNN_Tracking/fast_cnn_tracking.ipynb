{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fast Fiducial Marker Tracking using CNNs**\n",
    "\n",
    "*This works develops a convolutional neural network model (CNN) for tracking fiducial markers as fast as possible in order to enable real-time application in embedded systems. A comparison is made between classical methods for edge detection implemented in the OpenCV library.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiducial markers are used in many applications which is necessary to track an object in real time, such as flight testing, automobile testing, mixed reality and optical experiments. For having a predefined shape with fixed and distinct characteristics from the ambient, such as high contrast edges, they are easier to track than an arbitary object that it's glued on. Also in applications that require a higher precision and lower misidentification errors it is often used.\n",
    "\n",
    "There are several classical algorithms that are used for identifying many markers composed from crosses, circles, edges, lines, edges such as: Harris Corner detection, Shi-Tomasi Corner detection, SIFT, SUFT, FAST, BRIEF, ORB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary code such as dataset generation and image processing has been moved from this notebook to external python files so that this notebook would be more concise and be focused on the machine learning aspect of this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import urllib.request\n",
    "import cv2\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\") # or darkgrid\n",
    "#from jupyterthemes import jtplot\n",
    "#jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from libs_utils import gen_dataset, load_datafiles, save_datafiles, evaluate, train, calc_error, negative_samples, incorporate_dataset\n",
    "from libs_utils import svg_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "In this work a secchi disk was used, which encompasses an edge inside a circle, as show in below. The SIZE is defined in pixels. The parameters DEVICE selects the gpu if available and the RETRAIN flag trains the networks from scratch, instead of loading the lastest trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "SIZE = 24\n",
    "PADD = 4\n",
    "RETRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Error\n",
    "The first step was to create a model that estimates only the position of the markers given a cropped region which contains the center of the marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation\n",
    "\n",
    "The first dataset generated is a simpler one with markers with varying position, color, scale and rotation. The default parameters varies within a uniform distribution of 8 to 30 for the marker radius (in pixels) independently for each axis, 0 to 180 degrees to rotation, 0 to 90 for the background color, 0 to to 60 for the darker marker color and 60 to 165 for the brighter marker color.\n",
    "\n",
    "Given that the dataset occupies a few gigabytes in memory, for optimization puporses, it is keept loaded in memory throughout the training and evaluation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    imgs, pos = load_datafiles()\n",
    "    SIZE = imgs.shape[2]\n",
    "except:\n",
    "    imgs, pos = gen_dataset(320000, SIZE, SIZE, PADD) # already randomized\n",
    "    save_datafiles(imgs, pos)\n",
    "rng = np.random.default_rng(42)\n",
    "imgs = imgs.astype(np.float32)/255\n",
    "for i in range(imgs.shape[0]):\n",
    "     noise = rng.uniform(0, 0.01)\n",
    "     imgs[i, :, :, :] = np.clip(imgs[i] + rng.uniform(-noise, noise, size=imgs[i].shape), 0, 1)\n",
    "pos = torch.Tensor(pos.astype(np.float32)/SIZE).to(DEVICE)\n",
    "validation_size = 16000\n",
    "test_size = 16000\n",
    "train_size = imgs.shape[0] - validation_size - test_size\n",
    "train_dataset = TensorDataset(torch.Tensor(imgs[:train_size]).to(DEVICE), pos[:train_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "val_dataset = TensorDataset(torch.Tensor(imgs[train_size:-test_size]).to(DEVICE), pos[train_size:-test_size])\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "test_dataset = TensorDataset(torch.Tensor(imgs[-test_size:]).to(DEVICE), pos[-test_size:])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(8, 12),axes_pad=0.1)\n",
    "for i, ax in enumerate(grid):\n",
    "    ax.imshow(imgs[i, 0], cmap='gray',  vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's definition\n",
    "\n",
    "Those are the first three simpler models that were defined and had its hyperparameters manually selected during the validation.\n",
    "\n",
    "The SimpleConvOnlyPos was superseded by its Leaky counterpart that uses a LeakyReLU activation function. The wider model has more parameters that increase its accuracy at the expense of more processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvOnlyPos(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actv_func = F.relu\n",
    "        self.conv0 = nn.Conv2d(1, 4, 3)\n",
    "        self.conv1 = nn.Conv2d(4, 8, 5)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 5)\n",
    "        self.conv4 = nn.Conv2d(8, 4, 5)\n",
    "        self.conv5 = nn.Conv2d(4, 2, 6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.actv_func(self.conv0(x))\n",
    "        x = self.actv_func(self.conv1(x))\n",
    "        x = self.actv_func(self.conv2(x))\n",
    "        x = self.actv_func(self.conv3(x))\n",
    "        x = self.actv_func(self.conv4(x))\n",
    "        x = torch.flatten(self.conv5(x), 1)\n",
    "        return x\n",
    "\n",
    "class SimpleConvOnlyPosLeaky(SimpleConvOnlyPos):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actv_func = F.leaky_relu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(super().forward(x))\n",
    "\n",
    "class SimpleConvOnlyPosWider(SimpleConvOnlyPosLeaky):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 64, 3)\n",
    "        self.conv1 = nn.Conv2d(64, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 5)\n",
    "        self.conv4 = nn.Conv2d(8, 4, 5)\n",
    "        self.conv5 = nn.Conv2d(4, 2, 6)\n",
    "\n",
    "\n",
    "simpleConvOnlyPos = SimpleConvOnlyPos().to(DEVICE)\n",
    "simpleConvOnlyPosLeaky = SimpleConvOnlyPosLeaky().to(DEVICE)\n",
    "simpleConvOnlyPosWider = SimpleConvOnlyPosWider().to(DEVICE)\n",
    "onlyPosModels = [simpleConvOnlyPos, simpleConvOnlyPosLeaky, simpleConvOnlyPosWider]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "The training has an early stopping implemented to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in onlyPosModels:\n",
    "    name = model.__class__.__name__\n",
    "    log_run_num = len(glob.glob('saved_models/%s_*'%name)) + 1\n",
    "    n = '%s_%d' % (name, log_run_num)\n",
    "    if RETRAIN or log_run_num == 1:\n",
    "        print(n)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optim = Adam(model.parameters(), lr=0.001)\n",
    "        writer = SummaryWriter('tb_logs/%s' % n, flush_secs=1)\n",
    "        print('Training')\n",
    "        train_loss, val_loss = train(model, loss_fn, optim, train_dataloader, val_dataloader, epochs=500, tensorboard_writer=writer, path='saved_models/%s.pth'%n, patience=50)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('saved_models/%s_%d.pth' % (name, log_run_num - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in onlyPosModels:\n",
    "    outputs, mse = evaluate(model, nn.MSELoss(), val_dataloader)\n",
    "    print(model.__class__.__name__, '%.4f' % (np.sqrt(mse) * SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset augmentation with negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_imgs = negative_samples(imgs.shape[0], H=SIZE, W=SIZE, images_glob_path='./neg_imgs/*.jpg')\n",
    "train_dataloader2 = incorporate_dataset(neg_imgs[:train_size], train_dataset, batch_size=32)\n",
    "val_dataloader2 = incorporate_dataset(neg_imgs[train_size:-test_size], val_dataset, batch_size=128)\n",
    "np.random.seed(42)\n",
    "test_neg_imgs = negative_samples(test_size, H=SIZE, W=SIZE, images_glob_path='./neg_imgs_test/*.jpg')\n",
    "test_neg_imgs = neg_imgs[-test_size:]\n",
    "test_dataloader2 = incorporate_dataset(test_neg_imgs, test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs2 = train_dataloader2.dataset.tensors[0][:256].cpu().numpy()\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(8, 12),axes_pad=0.1)\n",
    "for i, ax in enumerate(grid):\n",
    "    ax.imshow(imgs2[i, 0], cmap='gray',  vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(SimpleConvOnlyPosLeaky):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv5 = nn.Conv2d(4, 3, 6)\n",
    "    \n",
    "    def load_from_onlypos_model(self, onlypos_model):\n",
    "        new_dict = onlypos_model.state_dict()\n",
    "        self.conv5.weight.data[:2, :, :, :] = new_dict['conv5.weight']\n",
    "        new_dict['conv5.weight'] = self.conv5.weight\n",
    "        self.conv5.bias.data[:2] = new_dict['conv5.bias']\n",
    "        new_dict['conv5.bias'] = self.conv5.bias\n",
    "        self.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning from previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvOnlyPosLeaky().to(DEVICE)\n",
    "model.load_state_dict(torch.load('saved_models/SimpleConvOnlyPosLeaky_1.pth'))\n",
    "sconv = SimpleConv()\n",
    "sconv.load_from_onlypos_model(model)\n",
    "sconv = sconv.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss definition for positional and classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCELoss()\n",
    "K = SIZE * SIZE\n",
    "def mse_bce(output, target):\n",
    "    return torch.mean((output[:,:2]-target[:,:2])**2*target[:,2:]*K) + bce(output[:,2], target[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask training for localization and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SimpleConv'\n",
    "log_run_num = len(glob.glob('saved_models/%s_*'%name)) + 1\n",
    "n = '%s_%d' % (name, log_run_num)\n",
    "if RETRAIN or log_run_num == 1:\n",
    "    optimizer_ce = torch.optim.Adam(sconv.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter('tb_logs/%s' % n)\n",
    "    train_loss, val_loss = train(sconv, mse_bce, optimizer_ce, train_dataloader2, val_dataloader2, epochs=500, tensorboard_writer=writer, path='saved_models/%s.pth'%n, patience=50)\n",
    "else:\n",
    "    sconv.load_state_dict(torch.load('saved_models/%s_%d.pth' % (name, log_run_num - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wider model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvWider(SimpleConvOnlyPosWider):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv5 = nn.Conv2d(4, 3, 6)\n",
    "    \n",
    "    def load_from_onlypos_model(self, onlypos_model):\n",
    "        new_dict = onlypos_model.state_dict()\n",
    "        self.conv5.weight.data[:2, :, :, :] = new_dict['conv5.weight']\n",
    "        new_dict['conv5.weight'] = self.conv5.weight\n",
    "        self.conv5.bias.data[:2] = new_dict['conv5.bias']\n",
    "        new_dict['conv5.bias'] = self.conv5.bias\n",
    "        self.load_state_dict(new_dict)\n",
    "        \n",
    "model = SimpleConvOnlyPosWider().to(DEVICE)\n",
    "model.load_state_dict(torch.load('saved_models/SimpleConvOnlyPosWider_1.pth'))\n",
    "sconvw = SimpleConvWider()\n",
    "sconvw.load_from_onlypos_model(model)\n",
    "sconvw = sconvw.to(DEVICE)\n",
    "\n",
    "name = 'SimpleConvWider'\n",
    "log_run_num = len(glob.glob('saved_models/%s_*'%name)) + 1\n",
    "n = '%s_%d' % (name, log_run_num)\n",
    "if RETRAIN or log_run_num == 1:\n",
    "    optimizer_ce = torch.optim.Adam(sconvw.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter('tb_logs/%s' % n)\n",
    "    train_loss, val_loss = train(sconvw, mse_bce, optimizer_ce, train_dataloader2, val_dataloader2, epochs=500, tensorboard_writer=writer, path='saved_models/%s.pth'%n, patience=50)\n",
    "else:\n",
    "    sconvw.load_state_dict(torch.load('saved_models/%s_%d.pth' % (name, log_run_num - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, H, W, device=DEVICE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding=2, stride=2)\n",
    "        #self.batch_norm = nn.BatchNorm2d(4)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 5, padding=2)\n",
    "        #self.conv4 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        #self.fc1 = nn.Linear(96, 32)\n",
    "        self.fc2 = nn.Linear(48, 16)\n",
    "        self.fc3 = nn.Linear(16, 3)\n",
    "        self.width = (torch.arange(0, W).reshape((1, 1, 1, W))/float(W)).to(device)\n",
    "        self.height = (torch.arange(0, H).reshape((1, 1, H, 1))/float(H)).to(device)\n",
    "        self.mean = (W*H)**0.5\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        c1 = self.conv1.weight.data\n",
    "        c1 = c1 / 2\n",
    "        c1[0, 0, ::2, ::2] = 1\n",
    "        c1[1, 0, ::2, 1::2] = 1\n",
    "        c1[2, 0, 1::2, ::2] = 1\n",
    "        c1[3, 0, 1::2, 1::2] = 1\n",
    "        self.conv1.weight.data = c1\n",
    "        c2 = self.conv2.weight.data\n",
    "        c2 = c2 / 2\n",
    "        c2[0, :4, :2, :2] = 1\n",
    "        c2[0, :4, 3:, :2] = -1\n",
    "        c2[0, :4, :2, 3:] = -1\n",
    "        c2[0, :4, 3:, 3:] = 1\n",
    "        c2[1, :4, :2, 2] = 1\n",
    "        c2[1, :4, 2, :2] = -1\n",
    "        c2[1, :4, 3:, 2] = 1\n",
    "        c2[1, :4, 2, 3:] = -1\n",
    "        c2[2, :4, :2, :] = 1\n",
    "        c2[2, :4, 3:, :] = -1\n",
    "        c2[3, :4, :2, :] = 1\n",
    "        c2[3, :4, 3:, :] = -1\n",
    "        self.conv2.weight.data = c2\n",
    "        c3 = self.conv3.weight.data\n",
    "        c3[:, :8, 2, 2] = 1\n",
    "        self.conv3.weight.data = c3\n",
    "    \n",
    "    def __image_forward__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #x = self.batch_norm(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        #x = F.leaky_relu(self.conv4(x))\n",
    "        x = torch.exp(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.__image_forward__(x)\n",
    "        sum_img = torch.sum(torch.sum(x, dim=2), dim=2)\n",
    "        H_activ = torch.sum(torch.sum(self.height * x, dim=2), dim=2) / sum_img\n",
    "        W_activ = torch.sum(torch.sum(self.width * x, dim=2), dim=2) / sum_img\n",
    "        x = torch.cat([sum_img/self.mean, H_activ, W_activ], dim=1)\n",
    "        #x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ConvNet'\n",
    "convnet_upsam = ConvNet(SIZE, SIZE).to(DEVICE)\n",
    "log_run_num = len(glob.glob('saved_models/%s_*'%name)) + 1\n",
    "n = '%s_%d' % (name, log_run_num)\n",
    "if RETRAIN or log_run_num == 1:\n",
    "    optimizer_ce = torch.optim.Adam(convnet_upsam.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter('tb_logs/%s' % n)\n",
    "    train_loss, val_loss = train(convnet_upsam, mse_bce, optimizer_ce, train_dataloader2, val_dataloader2, epochs=500, tensorboard_writer=writer, path='saved_models/%s.pth'%n, patience=50)\n",
    "else:\n",
    "    convnet_upsam.load_state_dict(torch.load('saved_models/%s_%d.pth' % (name, log_run_num - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetCompat(nn.Module):\n",
    "    def __init__(self, H, W, device=DEVICE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 5, padding=2)\n",
    "        #self.conv4 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        #self.fc1 = nn.Linear(96, 32)\n",
    "        self.fc2 = nn.Linear(48, 16)\n",
    "        self.fc3 = nn.Linear(16, 3)\n",
    "        self.width = (torch.arange(0, W).reshape((1, 1, 1, W))/float(W)).to(device)\n",
    "        self.height = (torch.arange(0, H).reshape((1, 1, H, 1))/float(H)).to(device)\n",
    "        self.mean = (W*H)**0.5\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        c1 = self.conv1.weight.data\n",
    "        c1 = c1 / 2\n",
    "        c1[0, 0, ::2, ::2] = 1\n",
    "        c1[1, 0, ::2, 1::2] = 1\n",
    "        c1[2, 0, 1::2, ::2] = 1\n",
    "        c1[3, 0, 1::2, 1::2] = 1\n",
    "        self.conv1.weight.data = c1\n",
    "        c2 = self.conv2.weight.data\n",
    "        c2 = c2 / 2\n",
    "        c2[0, :4, :2, :2] = 1\n",
    "        c2[0, :4, 3:, :2] = -1\n",
    "        c2[0, :4, :2, 3:] = -1\n",
    "        c2[0, :4, 3:, 3:] = 1\n",
    "        c2[1, :4, :2, 2] = 1\n",
    "        c2[1, :4, 2, :2] = -1\n",
    "        c2[1, :4, 3:, 2] = 1\n",
    "        c2[1, :4, 2, 3:] = -1\n",
    "        c2[2, :4, :2, :] = 1\n",
    "        c2[2, :4, 3:, :] = -1\n",
    "        c2[3, :4, :2, :] = 1\n",
    "        c2[3, :4, 3:, :] = -1\n",
    "        self.conv2.weight.data = c2\n",
    "        c3 = self.conv3.weight.data\n",
    "        c3[:, :8, 2, 2] = 1\n",
    "        self.conv3.weight.data = c3\n",
    "    \n",
    "    def __image_forward__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #x = self.batch_norm(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        #x = F.leaky_relu(self.conv4(x))\n",
    "        x = torch.exp(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.__image_forward__(x)\n",
    "        sum_img = torch.sum(torch.sum(x, dim=2), dim=2)\n",
    "        H_activ = torch.sum(torch.sum(self.height * x, dim=2), dim=2) / sum_img\n",
    "        W_activ = torch.sum(torch.sum(self.width * x, dim=2), dim=2) / sum_img\n",
    "        x = torch.cat([sum_img/self.mean, H_activ, W_activ], dim=1)\n",
    "        #x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ConvNetCompat'\n",
    "convnet = ConvNetCompat(SIZE, SIZE).to(DEVICE)\n",
    "log_run_num = len(glob.glob('saved_models/%s_*'%name)) + 1\n",
    "n = '%s_%d' % (name, log_run_num)\n",
    "if RETRAIN or log_run_num == 1:\n",
    "    optimizer_ce = torch.optim.Adam(convnet.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter('tb_logs/%s' % n)\n",
    "    train_loss, val_loss = train(convnet, mse_bce, optimizer_ce, train_dataloader2, val_dataloader2, epochs=500, tensorboard_writer=writer, path='saved_models/%s.pth'%n, patience=50)\n",
    "else:\n",
    "    convnet.load_state_dict(torch.load('saved_models/%s_%d.pth' % (name, log_run_num - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetExtended(nn.Module):\n",
    "    def __init__(self, convnet):\n",
    "        super().__init__()\n",
    "        self.h, self.w = convnet.height.shape[2], convnet.width.shape[3]\n",
    "        self.h2, self.w2 = self.h//2, self.w//2\n",
    "        self.convnet = convnet\n",
    "        self.sumconv = nn.Conv2d(1, 1, (self.h, self.w), stride=(self.h2, self.w2))\n",
    "        self.hconv = nn.Conv2d(1, 1, (self.h, self.w), stride=(self.h2, self.w2))\n",
    "        self.wconv = nn.Conv2d(1, 1, (self.h, self.w), stride=(self.h2, self.w2))\n",
    "        self.fconv2 = nn.Conv2d(48, 16, 1)\n",
    "        self.fconv3 = nn.Conv2d(16, 3, 1)    \n",
    "        self.sumconv.weight.data[:,:,:,:] = 1\n",
    "        self.hconv.weight.data[:,:,:,:] = convnet.height.data\n",
    "        self.wconv.weight.data[:,:,:,:] = convnet.width.data\n",
    "        self.fconv2.weight.data[:,:,0,0] = convnet.fc2.weight.data\n",
    "        self.fconv3.weight.data[:,:,0,0] = convnet.fc3.weight.data\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convnet.__image_forward__(x)\n",
    "        b, c, h, w = x.size()\n",
    "        rh, rw = int((h-self.h)/self.h2)+1, int((w-self.w)/self.w2)+1\n",
    "        x = x.view(-1, 1, h, w)\n",
    "        sum_img =  self.sumconv(x)\n",
    "        H_activ = (self.hconv(x)/sum_img).view(b, c, rh, rw)\n",
    "        W_activ = (self.wconv(x)/sum_img).view(b, c, rh, rw)\n",
    "        sum_img = (sum_img/self.convnet.mean).view(b, c, rh, rw)\n",
    "        x = torch.cat([sum_img, H_activ, W_activ], dim=1)\n",
    "        x = F.leaky_relu(self.fconv2(x))\n",
    "        x = torch.sigmoid(self.fconv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical methodologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "mask = np.zeros((SIZE, SIZE), dtype=np.uint8)\n",
    "mask[PADD:-PADD, PADD:-PADD] = 255\n",
    "DIFF_MAX = 4\n",
    "\n",
    "def cveval(cvfunc, imgs, t, param):\n",
    "  pos, dets, metrics = [], [], []\n",
    "  for i in range(imgs.shape[0]):\n",
    "      p, detect, metric = cvfunc((imgs[i, 0]*255).astype(np.uint8), np.round(t[i, :2]*SIZE-0.5), param)\n",
    "      pos.append(p)\n",
    "      dets.append(detect)\n",
    "      metrics.append(metric)\n",
    "  pos = np.array(pos)\n",
    "  dets = np.array(dets, dtype = bool)\n",
    "  metrics = np.array(metrics)\n",
    "  f1 = sklearn.metrics.f1_score(t[:, 2], dets)\n",
    "  rmse = np.sqrt(np.mean(np.square(t[:, :2] * SIZE - pos)[np.logical_and(dets, t[:, 2]==1), :]))\n",
    "  err = np.linalg.norm(t[:, :2] * SIZE - pos, axis=-1)[np.logical_and(dets, t[:, 2]==1)]\n",
    "  sigma_err = err.std()\n",
    "  return rmse, f1, np.hstack([pos, dets.reshape(-1, 1).astype(np.float64)]), metrics, sigma_err\n",
    "\n",
    "def subpix(img, ti, param):\n",
    "    pos = cv2.cornerSubPix(img, np.array([[[ti[0], ti[1]]]], dtype=np.float32), (8,8), (-1,-1), criteria).reshape(-1) + 0.5\n",
    "    detect = True\n",
    "    quality = 100 - np.linalg.norm(ti-pos)\n",
    "    if quality < param or np.all(pos == ti) or np.linalg.norm(ti - pos) > DIFF_MAX:\n",
    "        detect = False\n",
    "    return pos, detect, quality\n",
    "\n",
    "def shitomasi(img, ti, param):\n",
    "    corners, quality =  cv2.goodFeaturesToTrackWithQuality(img, 1, 0.01, 10, mask=mask, blockSize=4)\n",
    "    if corners is None or len(corners)==0 or np.linalg.norm(ti - corners.ravel()) > DIFF_MAX:\n",
    "        return np.array([0, 0.]), False, 0\n",
    "    return corners.ravel(), quality.ravel()[0] > param, quality.ravel()[0]\n",
    "\n",
    "def harris(img, ti, param):\n",
    "    corners, quality = cv2.goodFeaturesToTrackWithQuality(img, 1, 0.01, 10,  mask=mask, blockSize=4, useHarrisDetector=True)\n",
    "    if corners is None or len(corners)==0 or np.linalg.norm(ti - corners.ravel()) > DIFF_MAX:\n",
    "        return np.array([0, 0.]), False, 0\n",
    "    return corners.ravel(), quality.ravel()[0] > param, quality.ravel()[0]\n",
    "\n",
    "base_img = svg_gen(SIZE, SIZE, PADD, pos=[SIZE/2, SIZE/2], scale=[20, 20], rot=[120, 0], colors=[[150]*3, [40]*3, [60]*3])[0][:, :, 0]\n",
    "matcher = cv2.BFMatcher.create(cv2.NORM_L2, crossCheck=False)\n",
    "def matchers(kps, des, base_des):\n",
    "    if kps is None or len(kps) == 0:\n",
    "        return np.array([0, 0.]), False, 0\n",
    "    pos = np.mean(np.array([kp.pt for kp in kps]), axis=0)\n",
    "    matches = matcher.knnMatch(base_des, des, k=2)\n",
    "    if len(matches) == 0 or len(matches[0]) < 2:\n",
    "        metric = 0\n",
    "    else:\n",
    "        metric = np.sum([a.distance < 0.8 * b.distance  for a, b in matches])\n",
    "    return pos, metric > param, metric\n",
    "\n",
    "sift_config = cv2.SIFT_create(0, 3, 0.03, 10, 1)\n",
    "base_sift_kp, base_sift_des = sift_config.detectAndCompute(base_img, mask)\n",
    "def sift(img, ti, param):\n",
    "    kps, des = sift_config.detectAndCompute(img, mask)\n",
    "    return matchers(kps, des, base_sift_des)\n",
    "\n",
    "# surf = cv2.xfeatures2d.SURF_create(350)\n",
    "# base_surf_kp, base_surf_des = surf.detectAndCompute(base_img, mask)\n",
    "# def surf(img, ti, param):\n",
    "#     kps, des = surf.detectAndCompute(img, mask)\n",
    "#     return matchers(kps, des, base_surf_des)\n",
    "\n",
    "fast_config = cv2.FastFeatureDetector_create(threshold = 10, nonmaxSuppression = True , type=cv2.FAST_FEATURE_DETECTOR_TYPE_9_16)\n",
    "base_fast_kp = fast_config.detect(base_img, mask)\n",
    "def fast(img, ti, param):\n",
    "    kps = fast_config.detect(img, mask)\n",
    "    if kps is None or len(kps) == 0:\n",
    "        return np.array([0., 0]), False, 90\n",
    "    pos = np.array([kp.pt for kp in kps]).mean(axis=0)\n",
    "    quality = 100 - np.linalg.norm(ti-pos)\n",
    "    return pos, quality > param, quality\n",
    "\n",
    "def fit_param(quality, label):\n",
    "    qual_false = np.median(quality[label==0])\n",
    "    qual_true = np.median(quality[label==1])\n",
    "    param = np.mean([qual_false, qual_true])\n",
    "    return param\n",
    "\n",
    "methods = {\n",
    "    harris: 0.001,\n",
    "    shitomasi: 0.02,\n",
    "    subpix: 96.8,\n",
    "    sift: 0.5,\n",
    "    fast: 96\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method is first evaluated in the training set to find the optimal hyperparameters. Then they are evaluated on the validadation set in order to verify its performance and finally they are evaluated on the test set in order to get unbiased metrics. The mask, the block (and window) size, the \"k\" and other parameters where manually selected in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_dataloader2.dataset.tensors[0].cpu().numpy()\n",
    "train_res = train_dataloader2.dataset.tensors[1].cpu().numpy()\n",
    "val_imgs = val_dataloader2.dataset.tensors[0].cpu().numpy()\n",
    "val_res = val_dataloader2.dataset.tensors[1].cpu().numpy()\n",
    "test_imgs = test_dataloader2.dataset.tensors[0].cpu().numpy()\n",
    "test_res = test_dataloader2.dataset.tensors[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods have quality measures for the corner detection which are used to decide whether the image contains the center of the marker or not. The problem is that this corner detection technique has a high rate of false positives, and even the external circunference of the marker may be incorrectly identified as its center.\n",
    "For the cornerSubPix function, it has a metric of distancing between the original position and the refined, and there is also an internal metric of the error of the gradient that could also be used as a discriminator for marker detection. Nevertheless the positional method is simpler to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the training set was used to determine the values of those parameters that could discriminate between the markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    method_res = {}\n",
    "    for method, param in methods.items():\n",
    "        method_res[method] = cveval(method, train_imgs, train_res, param)\n",
    "        methods[method] = fit_param(method_res[method][3], train_res[:, 2])\n",
    "        print(methods)\n",
    "method_res_val = {}\n",
    "for method, param in methods.items():\n",
    "    method_res_val[method] = cveval(method, val_imgs, val_res, param)\n",
    "    print(method.__name__, '   \\t RMSE: %.4f / %.4f' % (method_res_val[method][0], method_res_val[method][-1]), ', F1: %.4f' % method_res_val[method][1], ', Acc: %.4f' % np.mean(method_res_val[method][2][:,-1] == val_res[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_res_test = {}\n",
    "for method, param in methods.items():\n",
    "    method_res_test[method] = cveval(method, test_imgs, test_res, param)\n",
    "    print(method.__name__, '   \\t RMSE: %.4f' % method_res_val[method][0], ', F1: %.4f' % method_res_val[method][1], ', Acc: %.4f' % np.mean(method_res_test[method][2][:,2] == test_res[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For models that only estimate the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in onlyPosModels:\n",
    "    outputs, mse = evaluate(model, nn.MSELoss(), test_dataloader)\n",
    "    print(model.__class__.__name__, '%.4f' % (np.sqrt(mse) * SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in [sconv, sconvw, convnet_upsam, convnet]:\n",
    "    outputs, mse = evaluate(model, mse_bce, test_dataloader2)\n",
    "    f1 = sklearn.metrics.f1_score(test_res[:, 2], outputs[:, 2]>0.5)\n",
    "    rmse = np.sqrt(np.mean(np.square(test_res[:, :2] -  outputs[:, :2])[np.logical_and(test_res[:, 2]==1, outputs[:, 2]>0.5), :])) * SIZE\n",
    "    err = np.linalg.norm(test_res[:, :2] -  outputs[:, :2], axis=-1)[np.logical_and(test_res[:, 2]==1, outputs[:, 2]>0.5),] * SIZE\n",
    "    sigma_err = err.std()\n",
    "    print(model.__class__.__name__, '   \\t RMSE: %.4f / %.4f' % (rmse, sigma_err), ', F1: %.6f' % f1, ', Acc: %.6f' % np.mean((outputs[:, 2]>0.5) == test_res[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rc('text', usetex = True ) # Enable tex for article\n",
    "plt.rcParams.update({'font.family': 'serif' })\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = pd.read_csv('./tb_logs/run-ConvNet_1-tag-Training Loss.csv')['Value'].values\n",
    "val_loss = pd.read_csv('./tb_logs/run-ConvNet_1-tag-Validation Loss.csv')['Value'].values\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.plot(list(range(len(train_loss))), train_loss)\n",
    "plt.plot(list(range(len(val_loss))), val_loss, '--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', \"Validation\"])\n",
    "plt.yscale('log')\n",
    "#plt.grid(color='gray', linestyle='-', linewidth=1)\n",
    "plt.savefig('traininghistory.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = method_res_test[subpix][2]\n",
    "o, mse = evaluate(sconvw, mse_bce, test_dataloader2)\n",
    "conv_rmse = np.sqrt(np.mean(np.square(test_res[:, :2] -  o[:, :2])[np.logical_and(test_res[:, 2]==1, o[:, 2]>0.5), :], axis=1)) * SIZE\n",
    "subpix_rmse = np.sqrt(np.mean(np.square(test_res[:, :2] * SIZE - s[:,:2])[np.logical_and(s[:,2]==1, test_res[:, 2]==1), :], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.displot({'Convolutional':conv_rmse[conv_rmse<1], 'Sub. Pix.': subpix_rmse[subpix_rmse<1]}, aspect=2.2)\n",
    "sns.move_legend(g, \"upper center\")\n",
    "plt.xlabel('Pixel Error (RMSE)')\n",
    "plt.savefig('errordist.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = (imgs[0][0]*255).astype(np.uint8)\n",
    "p = np.array([12., 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "harris(i, p, methods[harris])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "shitomasi(i, p, methods[shitomasi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "subpix(i, p, methods[subpix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sift(i, p, methods[sift])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fast(i, p, methods[fast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports\n",
    "\n",
    "The models should be exported and run with OpenVINO in order to assess its throughput (FPS).\n",
    "```\n",
    "python3 /opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py --input_model ./onxx/SimpleConv/sconv.onnx -o ~/openvino_models/\n",
    "path_to_openvino_build/inference_engine_samples_build/intel64/Release/benchmark_app -d GPU -i input_img.png -m ~/openvino_models/sconv.xml -pc -niter 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('onxx', exist_ok=True)\n",
    "os.makedirs('onxx/SimpleConv', exist_ok=True)\n",
    "os.makedirs('onxx/SimpleConvWider', exist_ok=True)\n",
    "os.makedirs('onxx/ConvNetCompat', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(train_dataloader2))[0]\n",
    "torch.onnx.export(sconv, X, \"onxx/SimpleConv/sconv.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(sconvw, X, \"onxx/SimpleConvWider/sconvw.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(convnet, X, \"onxx/ConvNetCompat/convnet.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in [('SimpleConv','sconv'), ('SimpleConvWider','sconvw'), ('ConvNetCompat','convnet')]:\n",
    "    mo_command = f\"\"\"mo --input_model \"onxx/{d}/{i}.onnx\" --compress_to_fp16 --output_dir \"onnx/{d}/\" \"\"\"\n",
    "    mo_result = %sx $mo_command\n",
    "    print(\"\\n\".join(mo_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in [('SimpleConv','sconv'), ('SimpleConvWider','sconvw'), ('ConvNetCompat','convnet')]:\n",
    "    benchmark_cmd = f\"\"\"~/openvino_cpp_samples_build/intel64/Release/benchmark_app -m \"onnx/{d}/{i}.xml\" -d GPU -t 30\"\"\"\n",
    "    benchmark_result = %sx $benchmark_cmd\n",
    "    print(\"\\n\".join(benchmark_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were refined on real images from the flight tests and were qualitatively evaluated on those videos. The real data and the refined model has a restricted access, due to data leakage concerns, but the model architecture is about the same as presented in this work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
